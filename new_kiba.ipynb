{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Drug_ID                                     Drug Target_ID  \\\n",
      "0  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    O00141   \n",
      "1  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    O14920   \n",
      "2  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    O15111   \n",
      "3  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    P00533   \n",
      "4  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    P04626   \n",
      "\n",
      "                                              Target     Y  \n",
      "0  MTVKTEAAKGTLTYSRMRGMVAILIAFMKQRRMGLNDFIQKIANNS...  11.1  \n",
      "1  MSWSPSLTTQTCGAWEMKERLGTGGFGNVIRWHNQETGEQIAIKQC...  11.1  \n",
      "2  MERPPGLRPGAGGPWEMRERLGTGGFGNVCLYQHRELDLKIAIKSC...  11.1  \n",
      "3  MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFED...  11.1  \n",
      "4  MELAALCRWGLLLALLPPGAASTQVCTGTDMKLRLPASPETHLDML...  11.1  \n"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "data = DTI(name = 'KIBA')\n",
    "print(data.get_data().head())\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Drug_ID', 'Drug', 'Target_ID', 'Target', 'Y'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "kiba_data = data.get_data()\n",
    "print(kiba_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.utils import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "import swifter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_pyg_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Atom features (You can expand this as needed)\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_type = atom.GetAtomicNum()  # Atomic number (basic feature)\n",
    "        formal_charge = atom.GetFormalCharge()  # Formal charge on the atom\n",
    "        atom_features.append([atom_type, formal_charge])  # Add more features if needed\n",
    "    \n",
    "    if len(atom_features) == 0:\n",
    "        return None\n",
    "\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    \n",
    "    # Edge features (You can expand this as needed)\n",
    "    edge_list = []\n",
    "    bond_types = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_type = bond.GetBondTypeAsDouble()  # Bond type as a numeric value\n",
    "\n",
    "        edge_list.append([i, j])\n",
    "        edge_list.append([j, i])\n",
    "        \n",
    "        bond_types.append([bond_type])\n",
    "        bond_types.append([bond_type])  # For undirected graph\n",
    "\n",
    "    if len(edge_list) == 0:\n",
    "        return None\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(bond_types, dtype=torch.float)  # Optional edge attributes\n",
    "\n",
    "    # Return PyG Data object with node features (x), edge_index, and edge features (edge_attr)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_compound_to_graph(compound_smiles):\n",
    "    \"\"\"\n",
    "    Convert compound SMILES string to a graph representation.\n",
    "    \n",
    "    :param compound_smiles: SMILES string of a compound.\n",
    "    :return: Graph representation (PyTorch Geometric Data object).\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(compound_smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_feature = [\n",
    "            atom.GetAtomicNum(), \n",
    "            atom.GetTotalNumHs(),  \n",
    "            atom.GetFormalCharge(), \n",
    "            atom.GetIsAromatic(),\n",
    "            atom.GetHybridization(),\n",
    "        ]\n",
    "        atom_features.append(atom_feature)\n",
    "\n",
    "    edge_index = []\n",
    "    edge_types = []\n",
    "    for bond in mol.GetBonds():\n",
    "        start_atom = bond.GetBeginAtomIdx()\n",
    "        end_atom = bond.GetEndAtomIdx()\n",
    "        bond_type = bond.GetBondTypeAsDouble()\n",
    "        \n",
    "        edge_index.append([start_atom, end_atom])\n",
    "        edge_index.append([end_atom, start_atom])\n",
    "        \n",
    "        edge_types.append([bond_type])\n",
    "        edge_types.append([bond_type])\n",
    "\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_types, dtype=torch.float)\n",
    "    \n",
    "    graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    \n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ProtBERT model and tokenizer\n",
    "model_name = 'Rostlab/prot_bert_bfd'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "protbert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Convert protein sequence to ProtBERT embedding\n",
    "def sequence_to_protbert_embedding(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = protbert_model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embedding.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create directories for saving tensors\n",
    "os.makedirs('graph_tensors', exist_ok=True)\n",
    "os.makedirs('protein_embeddings', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "protbert_model = protbert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to a new file after populating graphs and embeddings\n",
    "#kiba_data.to_pickle('processed_kiba_data.pkl')  # Save as a pickle file\n",
    "\n",
    "# Load it later\n",
    "kiba_data = pd.read_pickle('processed_kiba_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KIBADataset(Dataset):\n",
    "    def __init__(self, kiba_data):\n",
    "        self.kiba_data = kiba_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.kiba_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load graph tensor\n",
    "        graph_tensor = torch.load(f'graph_tensors/graph_{idx}.pt')\n",
    "        \n",
    "        # Load protein embedding tensor\n",
    "        protein_embedding = torch.load(f'protein_embeddings/protein_{idx}.pt')\n",
    "        \n",
    "        # Target label (Y value)\n",
    "        target = torch.tensor(self.kiba_data.iloc[idx]['Y'], dtype=torch.float)\n",
    "        \n",
    "        return graph_tensor, protein_embedding, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graphs, protein_embeddings, targets = zip(*batch)\n",
    "    batched_graphs = Batch.from_data_list(graphs)  # Batch graph data\n",
    "    protein_embeddings = torch.stack(protein_embeddings)  # Stack protein embeddings\n",
    "    targets = torch.tensor(targets)\n",
    "    \n",
    "    return batched_graphs, protein_embeddings, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset and DataLoader (assuming the KIBA dataset is already preprocessed)\n",
    "train_dataset = KIBADataset(kiba_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.data.batch.DataBatch'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Check the types of graphs in your DataLoader\n",
    "for batch in train_loader:\n",
    "    graphs, protein_embeddings, targets = batch\n",
    "    print(type(graphs), type(protein_embeddings))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Check types of embeddings\n",
    "for batch in train_loader:\n",
    "    graphs, protein_embeddings, targets = batch\n",
    "    print(type(protein_embeddings), type(protein_embeddings[0]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating graphs from SMILES strings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3528db47de54492887e754bb7b3cf0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/117657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Apply smiles_to_pyg_graph in parallel using swifter\n",
    "print(\"Generating graphs from SMILES strings...\")\n",
    "kiba_data['graph'] = kiba_data['Drug'].swifter.apply(convert_compound_to_graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating protein embeddings from sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30442c2c18364845b829dc71ff8b66b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/117657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply sequence_to_protbert_embedding in parallel using swifter\n",
    "print(\"Generating protein embeddings from sequences...\")\n",
    "kiba_data['protein_embedding'] = kiba_data['Target'].swifter.apply(sequence_to_protbert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[21, 5], edge_index=[2, 46], edge_attr=[46, 1])\n",
      "tensor([ 0.0306,  0.0243,  0.1363,  ..., -0.0868, -0.1145, -0.0130])\n"
     ]
    }
   ],
   "source": [
    "# Test SMILES to graph conversion\n",
    "sample_smiles = kiba_data['Drug'].iloc[0]\n",
    "print(convert_compound_to_graph(sample_smiles))\n",
    "\n",
    "# Test protein sequence to embedding conversion\n",
    "sample_sequence = kiba_data['Target'].iloc[0]\n",
    "print(sequence_to_protbert_embedding(sample_sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug_ID</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Target_ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Y</th>\n",
       "      <th>graph</th>\n",
       "      <th>protein_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>O00141</td>\n",
       "      <td>MTVKTEAAKGTLTYSRMRGMVAILIAFMKQRRMGLNDFIQKIANNS...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>[(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....</td>\n",
       "      <td>[tensor(0.0306), tensor(0.0243), tensor(0.1363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>O14920</td>\n",
       "      <td>MSWSPSLTTQTCGAWEMKERLGTGGFGNVIRWHNQETGEQIAIKQC...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>[(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....</td>\n",
       "      <td>[tensor(0.0306), tensor(0.0243), tensor(0.1363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>O15111</td>\n",
       "      <td>MERPPGLRPGAGGPWEMRERLGTGGFGNVCLYQHRELDLKIAIKSC...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>[(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....</td>\n",
       "      <td>[tensor(0.0306), tensor(0.0243), tensor(0.1363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>P00533</td>\n",
       "      <td>MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFED...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>[(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....</td>\n",
       "      <td>[tensor(0.0306), tensor(0.0243), tensor(0.1363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>P04626</td>\n",
       "      <td>MELAALCRWGLLLALLPPGAASTQVCTGTDMKLRLPASPETHLDML...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>[(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....</td>\n",
       "      <td>[tensor(0.0306), tensor(0.0243), tensor(0.1363...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Drug_ID                                     Drug Target_ID  \\\n",
       "0  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    O00141   \n",
       "1  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    O14920   \n",
       "2  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    O15111   \n",
       "3  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    P00533   \n",
       "4  CHEMBL1087421  COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2    P04626   \n",
       "\n",
       "                                              Target     Y  \\\n",
       "0  MTVKTEAAKGTLTYSRMRGMVAILIAFMKQRRMGLNDFIQKIANNS...  11.1   \n",
       "1  MSWSPSLTTQTCGAWEMKERLGTGGFGNVIRWHNQETGEQIAIKQC...  11.1   \n",
       "2  MERPPGLRPGAGGPWEMRERLGTGGFGNVCLYQHRELDLKIAIKSC...  11.1   \n",
       "3  MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFED...  11.1   \n",
       "4  MELAALCRWGLLLALLPPGAASTQVCTGTDMKLRLPASPETHLDML...  11.1   \n",
       "\n",
       "                                               graph  \\\n",
       "0  [(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....   \n",
       "1  [(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....   \n",
       "2  [(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....   \n",
       "3  [(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....   \n",
       "4  [(x, [tensor([6., 3., 0., 0., 4.]), tensor([8....   \n",
       "\n",
       "                                   protein_embedding  \n",
       "0  [tensor(0.0306), tensor(0.0243), tensor(0.1363...  \n",
       "1  [tensor(0.0306), tensor(0.0243), tensor(0.1363...  \n",
       "2  [tensor(0.0306), tensor(0.0243), tensor(0.1363...  \n",
       "3  [tensor(0.0306), tensor(0.0243), tensor(0.1363...  \n",
       "4  [tensor(0.0306), tensor(0.0243), tensor(0.1363...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiba_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: the trained DTIModel\n",
    "    - test_loader: DataLoader for the test set\n",
    "    - loss_fn: Loss function (e.g., MSELoss)\n",
    "    - device: torch device ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "    - Average test loss\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_data in test_loader:\n",
    "            graphs, protein_embeddings, targets = batch_data\n",
    "            graphs = graphs.to(device)\n",
    "            protein_embeddings = protein_embeddings.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "         # Forward pass, passing the batch index\n",
    "            out = dti_model(graphs, protein_embeddings, graphs.batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Average loss over all test samples\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    return avg_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117657 entries, 0 to 117656\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Drug_ID            117657 non-null  object \n",
      " 1   Drug               117657 non-null  object \n",
      " 2   Target_ID          117657 non-null  object \n",
      " 3   Target             117657 non-null  object \n",
      " 4   Y                  117657 non-null  float64\n",
      " 5   graph              117657 non-null  object \n",
      " 6   protein_embedding  117657 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "kiba_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom dataset and data loader functions\n",
    "class KIBADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        graph = row['graph']\n",
    "        protein_embedding = row['protein_embedding']\n",
    "        target = row['Y']\n",
    "        return graph, protein_embedding, target\n",
    "\n",
    "# Initialize dataset\n",
    "train_dataset = KIBADataset(kiba_data)\n",
    "\n",
    "# Assuming `kiba_data` is your entire dataset\n",
    "train_data, test_data = train_test_split(kiba_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = KIBADataset(train_data)\n",
    "test_dataset = KIBADataset(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    graphs, protein_embeddings, targets = zip(*batch)\n",
    "    batched_graphs = Batch.from_data_list(graphs)\n",
    "    protein_embeddings = torch.stack(protein_embeddings)\n",
    "    targets = torch.tensor(targets, dtype=torch.float)\n",
    "    return batched_graphs, protein_embeddings, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads, use_bias=True):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.Q = nn.Linear(in_dim, out_dim * num_heads, bias=use_bias)\n",
    "        self.K = nn.Linear(in_dim, out_dim * num_heads, bias=use_bias)\n",
    "        self.V = nn.Linear(in_dim, out_dim * num_heads, bias=use_bias)\n",
    "        self.fc = nn.Linear(out_dim * num_heads, out_dim)  # Final projection layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Linear projections for queries, keys, values\n",
    "        Q_h = self.Q(x)  # Shape: [num_nodes, num_heads * out_dim]\n",
    "        K_h = self.K(x)  # Shape: [num_nodes, num_heads * out_dim]\n",
    "        V_h = self.V(x)  # Shape: [num_nodes, num_heads * out_dim]\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q_h = Q_h.view(-1, self.num_heads, self.out_dim)  # Shape: [num_nodes, num_heads, out_dim]\n",
    "        K_h = K_h.view(-1, self.num_heads, self.out_dim)  # Shape: [num_nodes, num_heads, out_dim]\n",
    "        V_h = V_h.view(-1, self.num_heads, self.out_dim)  # Shape: [num_nodes, num_heads, out_dim]\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.einsum('bhd,bhd->bh', Q_h, K_h) / (self.out_dim ** 0.5)\n",
    "\n",
    "        # Apply softmax over the edges for each head\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Multiply attention scores with the values\n",
    "        out = V_h * attention.unsqueeze(-1)  # Shape: [num_nodes, num_heads, out_dim]\n",
    "\n",
    "        # Aggregate across the heads and reshape\n",
    "        out = out.view(-1, self.out_dim * self.num_heads)  # Shape: [num_nodes, num_heads * out_dim]\n",
    "\n",
    "        # Final projection to collapse heads back to out_dim\n",
    "        out = self.fc(out)  # Shape: [num_nodes, out_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "class GraphTransformerNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(GraphTransformerNet, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, hidden_dim)  # Transformation from input_dim -> hidden_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        self.multihead_attn = MultiHeadAttentionLayer(hidden_dim, hidden_dim, num_heads)\n",
    "\n",
    "        # Add graph convolutional layers (GCNConv)\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        # Final linear layer to project from hidden_dim to out_dim\n",
    "        self.final_layer = nn.Linear(hidden_dim, out_dim)  # Projection back to out_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial transformation of input node features\n",
    "        x = F.relu(self.input_fc(x))  # x is transformed from input_dim -> hidden_dim\n",
    "\n",
    "        # Apply graph convolutional layers\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Apply multi-head attention layer after the GCN layers\n",
    "        x = self.multihead_attn(x, edge_index)\n",
    "\n",
    "        # Project the result to the final output dimension\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DTIModel(nn.Module):\n",
    "    def __init__(self, graph_transformer, protein_embedding_dim, hidden_dim):\n",
    "        super(DTIModel, self).__init__()\n",
    "        self.graph_transformer = graph_transformer\n",
    "        self.protein_fc = nn.Linear(protein_embedding_dim, hidden_dim)\n",
    "        self.final_fc = nn.Linear(hidden_dim * 2, 1)  # Combine drug and protein features\n",
    "\n",
    "    def forward(self, graphs, protein_embeddings, batch):\n",
    "        # Get drug graph features from Graph Transformer\n",
    "        x = graphs.x  # Node features\n",
    "        edge_index = graphs.edge_index  # Edge index\n",
    "\n",
    "        drug_features = self.graph_transformer(x, edge_index)\n",
    "\n",
    "        # Apply mean pooling to graph node features\n",
    "        drug_features_pooled = global_mean_pool(drug_features, batch)  # Mean pool over the nodes for each graph\n",
    "        \n",
    "        \n",
    "        # Reduce protein embeddings to the same dimension\n",
    "        protein_features = F.relu(self.protein_fc(protein_embeddings))\n",
    "\n",
    "        #drug_features_pooled = drug_features.mean(dim=0) #for model summary\n",
    "        \n",
    "        # Combine both features (drug + protein)\n",
    "        combined_features = torch.cat([drug_features_pooled, protein_features], dim=-1)\n",
    "\n",
    "        # Final prediction\n",
    "        out = self.final_fc(combined_features)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # class GraphTransformerNet(nn.Module):\n",
    "# #     def __init__(self, input_dim, hidden_dim, out_dim, num_layers, dropout=0.1):\n",
    "# #         super(GraphTransformerNet, self).__init__()\n",
    "# #         self.layers = nn.ModuleList()\n",
    "# #         self.input_fc = nn.Linear(input_dim, hidden_dim)  # Transformation from 5 -> 128 features\n",
    "# #         self.num_layers = num_layers\n",
    "# #         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "# #         # Add graph convolutional layers (GCNConv or others as needed)\n",
    "# #         for _ in range(num_layers):\n",
    "# #             self.layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "# #         self.final_layer = GCNConv(hidden_dim, out_dim)  # Final layer to output dimension\n",
    "\n",
    "# #     def forward(self, x, edge_index):\n",
    "# #         # Initial transformation of input node features (e.g., atomic numbers)\n",
    "# #         x = F.relu(self.input_fc(x))  # x is transformed from 5D -> 128D\n",
    "        \n",
    "# #         # Apply the graph convolutional layers\n",
    "# #         for layer in self.layers:\n",
    "# #             x = F.relu(layer(x, edge_index))\n",
    "# #             x = self.dropout(x)\n",
    "\n",
    "# #         # Final graph convolution layer\n",
    "# #         x = self.final_layer(x, edge_index)\n",
    "        \n",
    "# #         return x\n",
    "\n",
    "# class GraphTransformerNet(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, out_dim, num_layers, num_heads=8, dropout=0.1):\n",
    "#         super(GraphTransformerNet, self).__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         self.input_fc = nn.Linear(input_dim, hidden_dim)  # Transformation from 5 -> 128 features\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_heads = num_heads\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         # Add graph convolutional layers (GCNConv or others as needed)\n",
    "#         for _ in range(num_layers):\n",
    "#             self.layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "#         # Multi-head attention layer\n",
    "#         self.multihead_attn = MultiHeadAttentionLayer(hidden_dim, hidden_dim, num_heads)\n",
    "\n",
    "#         # Final graph convolution layer to output dimension\n",
    "#         self.final_layer = GCNConv(hidden_dim, out_dim)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         # Initial transformation of input node features (e.g., atomic numbers)\n",
    "#         x = F.relu(self.input_fc(x))  # x is transformed from input_dim -> hidden_dim\n",
    "        \n",
    "#         # Apply the graph convolutional layers\n",
    "#         for layer in self.layers:\n",
    "#             x = F.relu(layer(x, edge_index))\n",
    "#             x = self.dropout(x)\n",
    "\n",
    "#         # Apply multi-head attention layer after the GCN layers\n",
    "#         x = self.multihead_attn(x, edge_index)\n",
    "\n",
    "#         # Final graph convolution layer\n",
    "#         x = self.final_layer(x, edge_index)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 100  # You can adjust this\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize DTI Model\n",
    "# Define model with correct arguments\n",
    "in_dim = 5  # Input dimension (e.g., atom features)\n",
    "hidden_dim = 128  # Hidden dimension for transformer layers\n",
    "out_dim = 128  # Output dimension from transformer layers\n",
    "num_heads = 8\n",
    "num_layers = 12  # Number of layers in the graph transformer\n",
    "dropout = 0.1\n",
    "protein_embedding_dim = 1024 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize GraphTransformerNet and DTIModel\n",
    "graph_transformer = GraphTransformerNet(in_dim, hidden_dim, out_dim,num_heads, num_layers, dropout).to(device)\n",
    "dti_model = DTIModel(graph_transformer, protein_embedding_dim, hidden_dim).to(device)  # Pass hidden_dim here\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(dti_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Avg Loss: 1.0938499811441618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Avg Loss: 0.714787721892225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Avg Loss: 0.7158366828538949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Avg Loss: 0.7152901258519683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Avg Loss: 0.7159166746120077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Avg Loss: 0.7165546642678438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Avg Loss: 0.7132910434068019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Avg Loss: 0.7158813619192241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Avg Loss: 0.7127568095081684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Avg Loss: 0.7149880324736653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Avg Loss: 0.7133726887404919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Avg Loss: 0.7122642323729297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Avg Loss: 0.7112233366495609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Avg Loss: 0.70837590270337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Avg Loss: 0.7088977608406811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Avg Loss: 0.709479608487548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Avg Loss: 0.7084490371188041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Avg Loss: 0.7080223110335405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Avg Loss: 0.709024003984813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Avg Loss: 0.7078859442699369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Avg Loss: 0.7078688636138518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Avg Loss: 0.706981605522836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Avg Loss: 0.7073676005806103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Avg Loss: 0.7079271285064686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Avg Loss: 0.7080382304880224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Avg Loss: 0.7069547850458822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Avg Loss: 0.7069157339698355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Avg Loss: 0.7071160144653148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Avg Loss: 0.707062168010111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Avg Loss: 0.7081355244974795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Avg Loss: 0.7067081532678501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Avg Loss: 0.7080790278844328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Avg Loss: 0.7073651463309978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Avg Loss: 0.7048777053696131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Avg Loss: 0.7058981478031925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Avg Loss: 0.7061244502187505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Avg Loss: 0.7068897550792657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Avg Loss: 0.7062398880861958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Avg Loss: 0.7052883181156228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Avg Loss: 0.7061360322830428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Avg Loss: 0.705421927726285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Avg Loss: 0.7063294597512034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Avg Loss: 0.7046167190705206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Avg Loss: 0.7046734937609134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Avg Loss: 0.70621199979821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Avg Loss: 0.7062813058687991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Avg Loss: 0.703788040442624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Avg Loss: 0.7042859899352433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Avg Loss: 0.7042252462126826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Avg Loss: 0.7048069543321063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Avg Loss: 0.7037923487016006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Avg Loss: 0.7036582242360656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Avg Loss: 0.7048376851761365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Avg Loss: 0.7040916169119238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Avg Loss: 0.703953132681665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Avg Loss: 0.7052330413920631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Avg Loss: 0.704677948521806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Avg Loss: 0.7048944579798254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Avg Loss: 0.7040899945948499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Avg Loss: 0.7057660726702473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Avg Loss: 0.7030926981825637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Avg Loss: 0.7037735815577204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Avg Loss: 0.7043788250685143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Avg Loss: 0.7043588170636114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Avg Loss: 0.7029625084238503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Avg Loss: 0.7036585587423161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Avg Loss: 0.702548815412314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Avg Loss: 0.7022728517443894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Avg Loss: 0.7017939069810406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Avg Loss: 0.7021545416173443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Avg Loss: 0.7035569844198908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Avg Loss: 0.7030411841035051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Avg Loss: 0.7027664857249857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Avg Loss: 0.7014754640810916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Avg Loss: 0.7023533479919003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Avg Loss: 0.7017858584470322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Avg Loss: 0.7018293808616399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Avg Loss: 0.7008649016433554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Avg Loss: 0.7008595068557257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Avg Loss: 0.7004353156722297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Avg Loss: 0.7017780157283485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Avg Loss: 0.7007540671001641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Avg Loss: 0.7013800591508966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Avg Loss: 0.7012498691313369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Avg Loss: 0.7008698466863865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Avg Loss: 0.7016270305938578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Avg Loss: 0.7017881336927738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Avg Loss: 0.7016898205745674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Avg Loss: 0.7015240429454236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Avg Loss: 0.7003001640243728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Avg Loss: 0.7012457099042322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Avg Loss: 0.7020125189083195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Avg Loss: 0.7024282247922115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Avg Loss: 0.7007481717105061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Avg Loss: 0.7003922407407083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Avg Loss: 0.7012512849341194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Avg Loss: 0.7011095807614738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Avg Loss: 0.7016466249522508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Avg Loss: 0.7014241894867896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Avg Loss: 0.7005498836729941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dti_model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Initialize tqdm progress bar for each epoch\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "    for batch_idx, batch_data in progress_bar:\n",
    "        graphs, protein_embeddings, targets = batch_data\n",
    "        graphs = graphs.to(device)\n",
    "        protein_embeddings = protein_embeddings.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, passing the batch index\n",
    "        out = dti_model(graphs, protein_embeddings, graphs.batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(out.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Update tqdm description with current loss for each batch\n",
    "        progress_bar.set_postfix({'Batch': batch_idx + 1, 'Loss': epoch_loss / (batch_idx + 1)})\n",
    "\n",
    "    # Print epoch-level statistics after each epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {epoch_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: the trained DTIModel\n",
    "    - test_loader: DataLoader for the test set\n",
    "    - loss_fn: Loss function (e.g., MSELoss)\n",
    "    - device: torch device ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "    - Average test loss\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_data in test_loader:\n",
    "            graphs, protein_embeddings, targets = batch_data\n",
    "            graphs = graphs.to(device)\n",
    "            protein_embeddings = protein_embeddings.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "         # Forward pass, passing the batch index\n",
    "            out = dti_model(graphs, protein_embeddings, graphs.batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Average loss over all test samples\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    return avg_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model_mse(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch_data in data_loader:\n",
    "            graphs, protein_embeddings, targets = batch_data\n",
    "            graphs = graphs.to(device)\n",
    "            protein_embeddings = protein_embeddings.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(graphs, protein_embeddings, graphs.batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out.squeeze(), targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and true targets\n",
    "            all_predictions.append(out.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Concatenate all predictions and targets across batches\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "    # Calculate MSE using sklearn\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "\n",
    "    # Print MSE\n",
    "    print(f\"Evaluation MSE: {mse}\")\n",
    "\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation MSE: 0.7019504308700562\n"
     ]
    }
   ],
   "source": [
    "# Assuming dti_model and test_loader are already defined\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "mse = evaluate_model_mse(dti_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTIModel(\n",
      "  (graph_transformer): GraphTransformerNet(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x GCNConv(128, 128)\n",
      "    )\n",
      "    (input_fc): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (final_layer): GCNConv(128, 128)\n",
      "  )\n",
      "  (protein_fc): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (final_fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Layer: graph_transformer.layers.0.bias | Size: torch.Size([128]) | Values : tensor([-0.0317, -0.0417], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.layers.0.lin.weight | Size: torch.Size([128, 128]) | Values : tensor([[ 4.1892e-02,  4.8769e-03,  3.7488e-02, -1.7063e-01, -1.2858e-01,\n",
      "         -5.4119e-02, -1.4338e-01, -1.3337e-01, -2.8230e-02,  4.6697e-02,\n",
      "          1.0246e-01, -5.4214e-02,  2.7489e-02,  2.5758e-02,  6.0505e-04,\n",
      "          6.7776e-02, -1.3753e-01,  6.6764e-02,  4.5799e-02,  9.4719e-02,\n",
      "          1.4857e-01, -8.5030e-02,  3.3389e-02, -8.1973e-02,  5.6698e-02,\n",
      "         -2.4369e-03,  9.9961e-02, -1.2343e-01, -3.7945e-02, -8.3937e-03,\n",
      "          1.3951e-01, -1.6888e-01,  1.1416e-01, -1.7106e-01, -1.0482e-01,\n",
      "         -1.0064e-01, -1.0586e-01,  7.0342e-02,  4.8563e-02,  1.5337e-02,\n",
      "         -6.6967e-02, -7.7638e-03, -1.1842e-01,  6.7928e-02,  5.8280e-02,\n",
      "          1.5297e-01,  4.1714e-02,  1.4410e-01, -1.2417e-01, -8.4525e-02,\n",
      "         -1.3108e-01,  2.5130e-02,  1.1143e-01, -1.1268e-01, -1.5548e-01,\n",
      "          9.7602e-02, -1.4956e-02,  8.9949e-02, -1.4493e-01, -1.4642e-01,\n",
      "          3.0992e-02, -5.7981e-02, -2.9761e-02, -1.6503e-01,  2.4737e-02,\n",
      "          8.9622e-02, -4.8765e-02, -1.1898e-01,  4.5858e-02, -8.1548e-02,\n",
      "          1.4734e-02, -1.3860e-01, -1.2934e-01, -5.7260e-02,  7.9860e-02,\n",
      "         -7.6554e-02, -9.2665e-02,  1.2438e-01, -1.2882e-01,  6.2368e-02,\n",
      "          6.1810e-02, -1.1952e-01,  1.4942e-01, -1.6646e-01, -6.1951e-02,\n",
      "          8.0758e-02,  9.7649e-02, -3.2031e-02, -2.0584e-02,  1.6181e-02,\n",
      "          1.2607e-02, -1.4166e-01,  3.5143e-02, -4.4220e-02, -1.8109e-02,\n",
      "         -3.0998e-02,  1.2413e-01, -4.1833e-02, -5.6899e-02, -1.8461e-01,\n",
      "         -4.3758e-02, -9.6746e-02,  1.5206e-01, -7.6379e-02, -1.0064e-01,\n",
      "          1.1846e-01,  1.8373e-02,  9.7131e-02,  1.5029e-02,  3.8323e-02,\n",
      "         -1.1643e-01, -7.6332e-02,  6.0932e-02,  5.5522e-03,  6.3947e-02,\n",
      "          7.3705e-02,  8.2764e-02, -5.0769e-02, -1.4511e-01, -1.4210e-01,\n",
      "          1.9543e-02,  2.7581e-02, -1.1755e-01,  8.3193e-02, -8.9004e-02,\n",
      "         -1.1665e-01, -9.0748e-02, -7.6062e-02],\n",
      "        [ 4.8127e-01,  1.6598e-01, -3.8810e-02,  3.6956e-01,  8.2086e-02,\n",
      "         -5.0699e-02, -5.8152e-02, -1.0449e-01,  2.7024e-01,  4.0404e-02,\n",
      "          1.1799e-01,  5.4050e-01, -9.1620e-02, -1.2415e-01,  7.9108e-02,\n",
      "          3.3758e-01, -3.5222e-01,  5.1594e-02, -1.2657e+00, -1.5265e-01,\n",
      "         -1.2868e-01, -9.2647e-02,  4.7094e-02, -6.1807e-02, -1.7319e-01,\n",
      "         -2.7655e-01,  4.9083e-02, -1.4029e-01,  8.0662e-02, -3.1589e-01,\n",
      "          7.8793e-02, -2.6800e-01,  5.2027e-02, -5.3402e-02,  1.4245e-01,\n",
      "          1.4219e-02,  1.4567e-01, -4.5415e-02, -9.9190e-02, -8.1133e-02,\n",
      "          4.5246e-03,  2.1728e-01, -2.2315e-02, -8.5082e-02, -8.8487e-02,\n",
      "          6.9026e-02, -8.6505e-02, -6.5887e-02,  4.6892e-02, -4.1330e-02,\n",
      "         -5.1107e-02, -5.6516e-02, -7.1104e-01,  1.9423e-02, -4.7935e-01,\n",
      "         -1.0160e-01, -1.0024e-01,  1.1400e-01,  5.1590e-02, -1.2015e-01,\n",
      "         -3.5676e-02, -5.2831e-02,  1.4701e-01, -2.0633e-01, -5.5121e-02,\n",
      "         -7.9393e-01,  1.5275e-01, -7.1324e-02, -5.5008e-01,  8.0323e-02,\n",
      "          3.5371e-02, -1.2008e+00,  1.4668e-01,  5.7401e-02, -1.1669e-01,\n",
      "         -1.2129e+00, -8.9592e-02,  2.1182e-01,  1.5497e-02, -3.7400e-02,\n",
      "         -8.9441e-03, -6.9684e-02,  9.6262e-02,  1.7165e-01,  2.7125e-01,\n",
      "         -1.3226e-01,  1.2365e-01,  2.2069e-01,  3.3656e-02,  4.1413e-01,\n",
      "          1.3217e-01,  2.5180e-01,  3.6806e-02,  2.3056e-02, -3.6976e-02,\n",
      "         -3.0177e-02,  7.5973e-01, -8.3615e-02,  7.0880e-03, -1.4766e-01,\n",
      "         -7.6952e-02,  8.3590e-04, -1.0930e-01,  6.6864e-02,  2.3323e-02,\n",
      "          5.0660e-02, -2.5543e-01,  2.9949e-02,  1.0437e-02, -3.8578e-02,\n",
      "         -1.6718e-01, -2.9478e-02,  1.5438e-01,  2.1999e-02,  2.6372e-01,\n",
      "         -4.2141e-01, -2.8034e-01, -7.1027e-02, -9.1955e-02,  1.8034e-02,\n",
      "         -4.3754e-01, -9.4442e-02, -3.8474e-01, -4.5522e-02,  2.1265e-01,\n",
      "          1.6532e-02,  9.3156e-03, -8.0881e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.layers.1.bias | Size: torch.Size([128]) | Values : tensor([ 0.0586, -0.1241], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.layers.1.lin.weight | Size: torch.Size([128, 128]) | Values : tensor([[-9.0814e-02, -2.6275e-02, -1.2115e-01, -3.6890e-01,  6.0169e-02,\n",
      "         -2.4428e-01,  1.8632e-01, -3.9976e-01, -2.3530e-02, -1.3761e-01,\n",
      "          2.2290e-01, -2.5505e-03, -2.2186e-02,  5.9487e-02, -1.3646e-01,\n",
      "         -3.8959e-02, -8.7388e-01, -3.0994e-01, -1.2252e-01, -1.1854e-01,\n",
      "          8.4880e-02,  1.8907e-01,  1.0574e-01,  1.4911e-01, -6.4785e-02,\n",
      "          8.3702e-02, -8.5561e-02, -5.2478e-02, -3.0389e-01, -1.0932e-01,\n",
      "         -5.7279e-02, -3.9287e-03,  4.7087e-02, -5.2804e-01,  1.7531e-01,\n",
      "         -2.0774e-01,  6.3274e-02,  1.1277e-02, -1.4464e-02,  2.0673e-01,\n",
      "         -7.0672e-04, -6.5114e-02,  1.1876e-01,  1.1727e-01, -1.5713e-01,\n",
      "         -1.4034e-01, -7.2405e-02, -9.0351e-02, -6.6568e-02,  1.2435e-01,\n",
      "         -5.7380e-03,  7.4364e-03, -1.4816e-01,  6.5201e-02, -1.5418e-01,\n",
      "          8.7861e-02,  2.4422e-02, -2.3845e-01,  4.0374e-02, -5.3821e-01,\n",
      "          1.7141e-02,  7.6487e-02,  2.3159e-02, -1.2415e-01,  3.5263e-02,\n",
      "          1.0526e-02,  7.4675e-02, -9.0524e-02, -5.4515e-02,  1.1844e-01,\n",
      "         -4.0704e-02,  8.0151e-02, -6.1323e-03,  6.4663e-02, -4.3763e-02,\n",
      "          8.3949e-02, -1.1739e-01,  4.8680e-02,  9.2309e-02,  2.4975e-01,\n",
      "         -7.8660e-02, -1.7680e-03, -1.5372e-01, -5.3757e-01,  1.2513e-01,\n",
      "         -4.5129e-02, -6.3923e-03, -2.1703e-02,  1.9551e-01, -1.4913e-01,\n",
      "          9.6594e-02, -3.9885e-03,  3.7579e-02,  1.8271e-01, -8.8170e-02,\n",
      "         -3.2038e-02,  7.3377e-03, -5.1050e-02,  4.4889e-02,  7.5217e-03,\n",
      "         -4.2350e-02, -9.1997e-02, -7.2441e-01, -4.7178e-01,  1.4886e-01,\n",
      "         -1.2118e-01,  3.0858e-02, -5.7885e-02, -9.7587e-02,  1.1488e-01,\n",
      "         -7.3016e-02,  2.2791e-01, -5.0623e-02,  1.2604e-01, -2.5708e-01,\n",
      "         -9.6995e-02, -5.4928e-02,  4.6359e-02, -2.2041e-02, -1.2393e-01,\n",
      "          1.1464e-01, -2.7007e-02,  5.5216e-02,  1.1368e-01, -6.0788e-01,\n",
      "          1.3573e-01,  6.3246e-02, -1.2259e-01],\n",
      "        [-8.4163e-02,  4.1328e-02, -1.2792e-02, -1.1325e-01, -8.0618e-02,\n",
      "          5.0582e-02,  7.6575e-02, -1.5332e-01,  1.0992e-01,  1.1061e-01,\n",
      "         -2.6497e-02,  4.1973e-02, -1.2140e-01, -1.7233e-02, -9.6806e-02,\n",
      "          6.1378e-02, -1.8603e-01, -1.2050e-01,  1.3989e-01, -5.2011e-02,\n",
      "         -5.8479e-02,  1.1637e-01,  4.3737e-03,  5.3719e-02, -1.2670e-01,\n",
      "          1.2046e-01, -1.4043e-01,  1.3799e-01,  1.1743e-01,  8.2635e-02,\n",
      "          1.2269e-01,  5.9797e-02,  5.8110e-02, -1.4178e-01, -1.5531e-01,\n",
      "          1.8658e-01, -1.0694e-01,  3.3155e-02, -2.6137e-02, -1.1081e-01,\n",
      "         -2.6516e-02, -2.6705e-02,  1.1189e-01,  2.1340e-03, -1.6265e-01,\n",
      "         -1.4347e-02,  3.9483e-03,  8.6968e-02,  1.2185e-01,  8.7670e-02,\n",
      "         -1.4983e-01,  1.2021e-01, -1.1356e-01, -1.4743e-01, -1.3059e-01,\n",
      "         -1.4329e-01,  1.1836e-02, -1.0421e-01,  2.2856e-02, -1.2369e-01,\n",
      "         -3.8646e-02,  1.1097e-01,  1.6034e-01,  1.0598e-01,  1.4306e-01,\n",
      "          2.8972e-03,  1.1395e-01,  4.8256e-02,  1.4270e-01,  3.2806e-02,\n",
      "          8.7080e-02, -2.0206e-02, -7.8905e-02, -7.3051e-02,  1.6680e-01,\n",
      "          1.0937e-01,  1.5826e-02,  7.8372e-02,  1.1221e-01, -6.3861e-02,\n",
      "          2.2244e-02, -4.2624e-02, -7.8087e-02, -1.2418e-01, -6.9437e-02,\n",
      "         -6.4695e-02, -8.4116e-02,  5.1463e-03, -9.9927e-02,  8.9634e-02,\n",
      "         -1.3947e-02,  5.4913e-02,  1.0547e-01,  1.1733e-01,  6.7818e-02,\n",
      "          1.2372e-01,  5.6250e-02,  9.3497e-02, -8.1777e-02, -1.5511e-01,\n",
      "          7.9426e-02,  9.6035e-02, -4.4119e-02, -1.9441e-01, -1.6718e-03,\n",
      "          2.1819e-02,  3.5080e-02,  1.3043e-02,  3.0182e-02,  1.2756e-02,\n",
      "          6.8317e-02, -5.3205e-02, -2.8688e-02, -1.3296e-01, -7.0855e-02,\n",
      "         -1.3154e-01,  8.1697e-02, -3.6272e-03, -3.9592e-02, -9.6019e-02,\n",
      "          1.1704e-02, -2.4913e-02, -1.0608e-01, -1.6237e-01, -1.2131e-01,\n",
      "          6.4715e-02, -1.3638e-01, -6.9544e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.layers.2.bias | Size: torch.Size([128]) | Values : tensor([-0.0514, -0.1155], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.layers.2.lin.weight | Size: torch.Size([128, 128]) | Values : tensor([[-1.8838e-02,  1.4320e-01,  9.5338e-02, -1.7894e-01,  4.0250e-02,\n",
      "         -2.5298e-02,  1.3496e-01,  3.7346e-02, -1.0555e-01, -8.4292e-02,\n",
      "          3.8016e-02, -1.6252e-01, -7.6507e-02, -6.7338e-02, -2.5464e-01,\n",
      "         -2.2301e-01,  1.1198e-01, -1.7092e-01,  1.1752e-01, -2.1924e-01,\n",
      "         -9.1629e-02, -1.5652e-01, -3.4434e-01, -5.0699e-01, -2.8859e-01,\n",
      "          1.1678e-01,  5.5108e-03,  1.9028e-01, -1.9866e-01, -9.1569e-02,\n",
      "         -7.0576e-02, -1.1829e-01, -1.3122e-01,  4.4033e-02,  6.6560e-02,\n",
      "          1.0621e-01,  5.0405e-02, -1.3340e-01, -1.0816e-02, -3.9248e-01,\n",
      "         -7.1784e-02, -4.7255e-01, -4.7423e-02, -2.2460e-01, -7.6700e-01,\n",
      "          8.6624e-02, -1.2312e-01, -6.3404e-02, -1.3991e-01,  1.0420e-01,\n",
      "         -3.9968e-01, -2.6659e-02, -2.8830e-01,  1.1587e-01,  5.9218e-02,\n",
      "          1.1771e-02,  1.1807e-01, -1.8878e-01, -7.0648e-01, -2.1266e-01,\n",
      "          1.2305e-01, -3.6240e-02, -4.0158e-02, -4.0172e-01, -2.0807e-01,\n",
      "         -3.5800e-02,  1.1383e-01,  1.5611e-01,  2.2989e-01, -3.1143e-01,\n",
      "          5.5677e-02, -1.5665e-01,  3.8923e-02,  1.5664e-01, -1.5845e-01,\n",
      "         -2.6951e-01,  1.2802e-01, -1.0421e-01,  8.6946e-02, -1.3951e-01,\n",
      "          7.3682e-02,  2.9708e-02, -2.4247e-01,  3.6715e-02,  4.1824e-02,\n",
      "          5.6971e-02, -1.5442e-02, -3.0100e-02, -9.2570e-02,  5.3364e-02,\n",
      "          6.4817e-02, -9.2230e-02,  8.7860e-02,  6.4935e-02, -1.2893e-01,\n",
      "         -4.5446e-01, -4.3851e-02,  6.9943e-02, -1.4361e-01,  4.1927e-03,\n",
      "          4.3995e-02, -9.0182e-02, -6.3673e-02, -1.3370e-01, -2.0169e-01,\n",
      "          2.7868e-02, -2.4862e-01, -1.4608e-01,  7.6261e-02, -1.9977e-01,\n",
      "         -2.5570e-02,  1.1175e-02, -1.1793e-01, -5.0529e-02,  1.0662e-01,\n",
      "          3.7263e-02, -2.8578e-01, -8.9474e-02,  1.1382e-01,  1.2836e-01,\n",
      "         -1.0386e-01, -1.3251e-01, -7.9404e-02, -3.0039e-01, -5.2905e-01,\n",
      "         -1.9808e-01, -2.2658e-01,  9.2196e-02],\n",
      "        [-1.7567e-01, -4.6410e-02, -3.5999e-02, -1.5603e-01, -8.9382e-03,\n",
      "         -7.1732e-02, -1.5444e-01,  3.0156e-02, -4.8096e-02, -1.0557e-02,\n",
      "         -2.0023e-02,  6.1708e-02, -1.1523e-01,  1.0140e-01,  3.2648e-02,\n",
      "          2.6264e-02, -1.9834e-02, -7.5234e-04, -7.9690e-02,  1.1328e-02,\n",
      "         -2.9391e-02, -1.7743e-01, -2.3436e-02, -4.4078e-02,  8.5468e-02,\n",
      "          7.7465e-02, -1.7967e-01,  5.2051e-02,  3.5887e-02, -5.5632e-02,\n",
      "          6.5748e-02,  7.2967e-02, -1.1968e-01,  5.3330e-02, -1.3402e-01,\n",
      "         -1.2780e-01, -1.1695e-01,  7.3778e-02,  2.9589e-02, -3.4348e-02,\n",
      "         -1.3857e-01,  1.2062e-01, -5.2656e-02, -1.7386e-02,  1.5403e-02,\n",
      "         -6.3640e-02, -6.2179e-03,  1.2043e-01, -1.9406e-03, -1.5712e-01,\n",
      "          5.6585e-02, -2.5010e-02, -1.9688e-01, -1.0484e-01, -8.6887e-02,\n",
      "         -2.4820e-01, -5.7780e-02,  1.1686e-01, -2.3980e-01,  2.2822e-02,\n",
      "          1.0281e-01,  2.5429e-02,  2.0596e-02,  5.3489e-02,  1.0991e-01,\n",
      "         -2.0068e-01, -1.4064e-01,  2.5610e-02, -1.7743e-01, -1.7449e-03,\n",
      "         -1.3229e-01, -6.2681e-02, -5.8424e-02, -9.4032e-02,  2.2061e-02,\n",
      "          7.9112e-02,  1.6976e-02, -2.2727e-02,  5.8489e-02, -6.0327e-03,\n",
      "         -1.4877e-01,  1.2139e-01, -7.5079e-02,  1.4814e-02, -1.9149e-01,\n",
      "         -2.0143e-01,  1.0542e-01,  3.8002e-02, -8.3015e-02, -1.5293e-01,\n",
      "         -1.6927e-01, -6.2208e-02, -1.5899e-01, -1.1179e-01, -9.9271e-02,\n",
      "         -1.6000e-01,  1.1296e-01,  8.0432e-02,  6.4920e-02, -5.9201e-02,\n",
      "         -1.9922e-01,  4.5087e-02, -8.8758e-02, -1.8767e-01,  5.9322e-02,\n",
      "          4.1966e-03, -8.7120e-02,  9.7869e-02,  6.4789e-02,  1.0419e-02,\n",
      "         -5.0458e-02, -6.3223e-02, -1.8346e-01,  1.4184e-01,  1.1232e-01,\n",
      "         -8.7461e-02,  5.7198e-02, -1.7656e-01,  4.5582e-02,  2.4112e-02,\n",
      "          8.7642e-02,  3.9894e-02, -2.5195e-03, -1.6992e-01,  5.5580e-02,\n",
      "         -2.1361e-01,  4.6947e-02, -1.3839e-01]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.input_fc.weight | Size: torch.Size([128, 5]) | Values : tensor([[-0.2288, -0.4750,  0.2215, -0.5147,  0.4328],\n",
      "        [ 0.0510,  0.6514, -1.2039,  0.6200, -0.2309]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.input_fc.bias | Size: torch.Size([128]) | Values : tensor([ 0.4519, -0.3253], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.final_layer.bias | Size: torch.Size([128]) | Values : tensor([0.0471, 0.0028], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: graph_transformer.final_layer.lin.weight | Size: torch.Size([128, 128]) | Values : tensor([[ 0.2174,  0.1711,  0.1387, -0.1552, -0.0948, -0.1154, -0.0476,  0.3701,\n",
      "          0.0368, -0.1506,  0.0416, -0.0630,  0.1229, -0.0015,  0.0044,  0.0905,\n",
      "          0.0524, -0.0553,  0.0716,  0.0950,  0.0805, -0.0282, -0.0292, -0.0034,\n",
      "         -0.1708,  0.1766, -0.1162, -0.0582, -0.2547,  0.1072, -0.0527,  0.0210,\n",
      "          0.0696,  0.0631, -0.3180, -0.1447,  0.0524,  0.1045, -0.0040,  0.2831,\n",
      "          0.0715,  0.0628, -0.0806, -0.0830,  0.0049,  0.0552, -0.1068,  0.0367,\n",
      "          0.2671,  0.0301,  0.0950, -0.1339,  0.0330, -0.1578, -0.0273,  0.1077,\n",
      "         -0.0472, -0.0798,  0.0237, -0.1028,  0.0983,  0.0531, -0.1193, -0.3201,\n",
      "         -0.2707,  0.0418, -0.1277,  0.1409, -0.0827, -0.1001, -0.0524,  0.0594,\n",
      "          0.5396,  0.0540,  0.0919,  0.2807,  0.0514,  0.1112, -0.0556,  0.0764,\n",
      "          0.2625, -0.0734,  0.0983,  0.0012, -0.0603,  0.0543,  0.1036, -0.1518,\n",
      "         -0.0443,  0.1382,  0.0788, -0.0458, -0.0451, -0.0669,  0.0065,  0.1046,\n",
      "          0.0875,  0.2664, -0.0425, -0.0776,  0.1055, -0.0666, -0.0463, -0.0723,\n",
      "          0.0691,  0.0797,  0.1862,  0.0135,  0.1180, -0.0408,  0.0787,  0.4258,\n",
      "          0.0034,  0.0428,  0.0630,  0.1241, -0.0093,  0.0652,  0.0759, -0.2091,\n",
      "         -0.0107, -0.0207, -0.1436,  0.3686, -0.1854, -0.1097, -0.1247,  0.1177],\n",
      "        [ 0.1349, -0.0817, -0.0238, -0.1093, -0.1048, -0.0998,  0.0127, -0.0612,\n",
      "         -0.1541,  0.0121, -0.0897, -0.1050,  0.1025,  0.0176, -0.1643,  0.0103,\n",
      "          0.0524,  0.0690, -0.2031, -0.1123,  0.0961,  0.0242,  0.0537, -0.0514,\n",
      "         -0.1701,  0.1070,  0.0576,  0.0180, -0.0236,  0.1022,  0.0837, -0.0447,\n",
      "          0.0385, -0.0368,  0.0796,  0.0285,  0.0515, -0.1087, -0.0585,  0.0952,\n",
      "          0.0777, -0.0419, -0.0309,  0.1179, -0.1273, -0.0686, -0.1681, -0.2095,\n",
      "          0.0145,  0.0185,  0.1212, -0.1682,  0.1319, -0.0532, -0.1001, -0.0233,\n",
      "          0.0652, -0.0610,  0.0260, -0.0524,  0.0721, -0.1778, -0.0769, -0.0888,\n",
      "          0.0844, -0.0073, -0.0288, -0.0597,  0.0072, -0.1157,  0.0140,  0.0641,\n",
      "         -0.0270,  0.0671, -0.1323,  0.0893, -0.1066, -0.0242,  0.1360,  0.0021,\n",
      "         -0.1174, -0.1607, -0.0395, -0.1244, -0.1353, -0.0517,  0.1016, -0.0021,\n",
      "          0.0836, -0.1082, -0.0970,  0.0855, -0.1695, -0.0848,  0.0946, -0.0642,\n",
      "         -0.0064,  0.1257,  0.0615, -0.1008,  0.0330,  0.0287, -0.0921, -0.0017,\n",
      "          0.0088,  0.0759,  0.1641,  0.2532,  0.0162, -0.1227, -0.0794,  0.1215,\n",
      "         -0.0156, -0.0437,  0.0411,  0.0262,  0.0415, -0.2459, -0.1666,  0.2057,\n",
      "          0.0465,  0.0178, -0.0060,  0.0480,  0.0798,  0.0268,  0.0009, -0.0439]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: protein_fc.weight | Size: torch.Size([128, 1024]) | Values : tensor([[-2.4887e-02, -2.6447e-02, -3.4265e-03,  ...,  3.0301e-02,\n",
      "         -8.7518e-05,  2.1869e-02],\n",
      "        [-2.3048e-02,  9.7296e-03, -1.2956e-02,  ...,  8.4823e-03,\n",
      "         -1.4389e-02, -2.7087e-03]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: protein_fc.bias | Size: torch.Size([128]) | Values : tensor([ 0.0009, -0.0108], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: final_fc.weight | Size: torch.Size([1, 256]) | Values : tensor([[ 9.6631e-02, -1.1424e-03,  3.9615e-05,  3.6600e-04, -1.8467e-03,\n",
      "          2.2578e-01, -7.0099e-04,  1.1506e-01, -9.7624e-04, -2.6805e-03,\n",
      "         -6.6326e-04,  1.1181e-01, -2.7596e-03,  1.7415e-05,  1.2424e-01,\n",
      "          3.9112e-04,  1.7325e-01,  1.0823e-01,  1.6853e-03, -1.8567e-03,\n",
      "          6.7876e-04,  7.1536e-04,  2.4477e-03,  2.4701e-03,  2.9096e-04,\n",
      "          1.9228e-03, -2.0759e-03, -9.8063e-02, -4.4217e-03,  2.8533e-03,\n",
      "         -1.7871e-04, -9.7928e-02, -5.0410e-02,  1.7326e-01, -2.7611e-03,\n",
      "          1.9396e-01,  2.8190e-03,  1.4501e-01,  3.3030e-03, -3.3715e-03,\n",
      "          8.7361e-02,  1.7830e-01, -9.8240e-04, -1.2876e-03, -2.3968e-01,\n",
      "         -1.4150e-03, -1.5107e-02, -7.4970e-02, -3.1659e-03,  2.7619e-04,\n",
      "         -8.0042e-04, -1.8645e-03, -1.2901e-04,  3.2748e-03,  3.0082e-03,\n",
      "          1.5768e-01,  2.8122e-03, -2.0707e-01, -7.2829e-02,  2.4863e-03,\n",
      "          1.0185e-01,  2.2422e-03, -1.6022e-03,  3.0756e-03,  2.7883e-03,\n",
      "          1.3764e-02,  3.7171e-02,  1.8600e-01,  1.6894e-01, -2.2049e-03,\n",
      "          2.0639e-03,  1.5502e-01,  5.5087e-04, -1.1436e-02,  8.5320e-02,\n",
      "          1.5263e-03, -3.5585e-03, -2.9616e-02, -5.9012e-03,  1.4884e-03,\n",
      "          3.5596e-03,  2.1741e-03,  1.8778e-01,  3.6532e-04,  1.5186e-01,\n",
      "          4.9385e-03,  7.1799e-02,  2.5775e-04,  4.2040e-03,  7.2405e-04,\n",
      "         -1.4826e-01, -2.1869e-03,  4.7588e-03,  2.6477e-03, -6.5113e-02,\n",
      "         -1.7385e-01, -1.0339e-01, -5.3540e-03, -1.3770e-03,  3.5298e-04,\n",
      "         -9.5017e-04, -9.9151e-05,  1.4721e-01, -1.4355e-01, -1.7272e-01,\n",
      "          2.3907e-03, -1.1448e-01,  3.3419e-04, -1.1286e-03,  1.2625e-01,\n",
      "         -4.0590e-03,  2.6132e-03, -3.6875e-03, -1.3873e-05,  4.2149e-03,\n",
      "         -7.6818e-05, -8.5443e-04, -7.0476e-02, -1.1688e-03,  1.0493e-04,\n",
      "          7.6250e-04, -4.4955e-03, -2.2812e-03, -5.8479e-04,  4.4718e-03,\n",
      "          1.2197e-01,  2.0711e-03, -7.7242e-06,  2.3849e-02, -4.6162e-02,\n",
      "         -2.9419e-02, -3.9675e-02,  5.5240e-03,  6.6798e-02,  6.5051e-02,\n",
      "         -1.4604e-02,  9.4248e-02,  5.6918e-02, -8.1608e-04, -5.2826e-02,\n",
      "          2.3036e-02, -5.9258e-02, -5.8345e-03,  6.0235e-03,  4.6510e-02,\n",
      "          3.6004e-02, -5.1419e-02, -1.0449e-02,  8.9713e-02,  8.0645e-03,\n",
      "         -4.1909e-02, -5.1012e-02, -3.4796e-02, -4.3597e-02, -5.6835e-02,\n",
      "          1.5914e-02,  7.3704e-02, -2.1604e-02,  1.2064e-02, -3.6459e-02,\n",
      "          2.0757e-02, -4.8497e-02, -5.4374e-02,  4.5248e-03,  5.2559e-02,\n",
      "          4.5310e-03,  5.9753e-03,  6.3390e-02,  4.4155e-02, -2.1953e-02,\n",
      "          2.9301e-03,  3.4797e-02,  5.7300e-02, -1.1196e-02,  5.0473e-02,\n",
      "          3.6527e-02,  1.1468e-01,  5.0881e-02, -4.2928e-02, -3.5820e-02,\n",
      "         -1.3781e-02, -2.8774e-02,  2.9539e-02,  1.1525e-02, -2.9988e-02,\n",
      "         -3.7045e-03, -2.9707e-02,  2.1976e-02,  5.8181e-02, -4.8853e-02,\n",
      "         -2.8897e-03,  1.0324e-01, -5.9270e-02, -4.3145e-02, -4.1392e-02,\n",
      "          3.8199e-02,  6.2997e-02,  1.7173e-02,  8.8961e-02,  5.4399e-02,\n",
      "          5.6500e-02, -3.5131e-02, -5.3363e-02, -4.6219e-02,  2.8133e-02,\n",
      "         -2.0727e-02, -2.1104e-02,  1.0638e-01,  1.4332e-02,  5.5798e-02,\n",
      "          2.5480e-02,  2.3662e-03, -2.3010e-02, -4.7113e-02,  5.8481e-02,\n",
      "          7.1879e-02,  3.4226e-03, -3.7238e-02,  5.0356e-02,  4.5532e-02,\n",
      "          4.1583e-02, -1.7192e-02, -3.2486e-02, -1.6014e-02,  4.6060e-02,\n",
      "          1.0771e-01,  5.3941e-02,  3.1940e-02,  1.6933e-02,  2.7498e-02,\n",
      "         -4.2995e-02, -4.6109e-02,  5.2889e-03,  4.5315e-02, -1.5551e-02,\n",
      "         -2.7878e-02,  1.1686e-01, -1.6321e-02,  3.4721e-02,  1.2003e-02,\n",
      "         -5.5660e-03,  1.1756e-01, -3.6471e-02,  3.8086e-02, -5.2622e-02,\n",
      "          6.0902e-02,  2.7848e-02, -2.1230e-02,  1.0919e-01, -2.2179e-02,\n",
      "          2.8251e-02,  2.3443e-02, -3.7752e-02,  5.2277e-02,  5.6105e-02,\n",
      "          8.6838e-03]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: final_fc.bias | Size: torch.Size([1]) | Values : tensor([0.1809], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture\n",
    "print(dti_model)\n",
    "\n",
    "# If you want to print more detailed info about the model, including parameters:\n",
    "for name, param in dti_model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DTIModel(\n",
       "  (graph_transformer): GraphTransformerNet(\n",
       "    (input_fc): Linear(in_features=5, out_features=128, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GCNConv(128, 128)\n",
       "    )\n",
       "    (multihead_attn): MultiHeadAttentionLayer(\n",
       "      (Q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (K): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (V): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (fc): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    )\n",
       "    (final_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (protein_fc): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (final_fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dti_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTIModel(\n",
      "  (graph_transformer): GraphTransformerNet(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x GCNConv(128, 128)\n",
      "    )\n",
      "    (input_fc): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (final_layer): GCNConv(128, 128)\n",
      "  )\n",
      "  (protein_fc): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (final_fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This will print a basic summary of the model\n",
    "print(dti_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "\n",
      "Detailed Parameter Information:\n",
      "======================================================================================\n",
      "Layer                                    Size                           Requires Grad\n",
      "--------------------------------------------------------------------------------------\n",
      "graph_transformer.input_fc.weight        torch.Size([128, 5])           True\n",
      "graph_transformer.input_fc.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.0.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.0.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.1.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.1.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.2.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.2.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.3.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.3.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.4.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.4.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.5.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.5.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.6.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.6.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.7.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.7.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.8.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.8.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.9.bias          torch.Size([128])              True\n",
      "graph_transformer.layers.9.lin.weight    torch.Size([128, 128])         True\n",
      "graph_transformer.layers.10.bias         torch.Size([128])              True\n",
      "graph_transformer.layers.10.lin.weight   torch.Size([128, 128])         True\n",
      "graph_transformer.layers.11.bias         torch.Size([128])              True\n",
      "graph_transformer.layers.11.lin.weight   torch.Size([128, 128])         True\n",
      "graph_transformer.multihead_attn.Q.weight torch.Size([1024, 128])        True\n",
      "graph_transformer.multihead_attn.Q.bias  torch.Size([1024])             True\n",
      "graph_transformer.multihead_attn.K.weight torch.Size([1024, 128])        True\n",
      "graph_transformer.multihead_attn.K.bias  torch.Size([1024])             True\n",
      "graph_transformer.multihead_attn.V.weight torch.Size([1024, 128])        True\n",
      "graph_transformer.multihead_attn.V.bias  torch.Size([1024])             True\n",
      "graph_transformer.multihead_attn.fc.weight torch.Size([128, 1024])        True\n",
      "graph_transformer.multihead_attn.fc.bias torch.Size([128])              True\n",
      "graph_transformer.final_layer.weight     torch.Size([128, 128])         True\n",
      "graph_transformer.final_layer.bias       torch.Size([128])              True\n",
      "protein_fc.weight                        torch.Size([128, 1024])        True\n",
      "protein_fc.bias                          torch.Size([128])              True\n",
      "final_fc.weight                          torch.Size([1, 256])           True\n",
      "final_fc.bias                            torch.Size([1])                True\n",
      "======================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the basic model summary\n",
    "print(\"Model Summary:\")\n",
    "\n",
    "\n",
    "# Print detailed parameter information\n",
    "print(\"\\nDetailed Parameter Information:\")\n",
    "print(\"=\"*86)\n",
    "print(f\"{'Layer':<40} {'Size':<30} {'Requires Grad'}\")\n",
    "print(\"-\"*86)\n",
    "\n",
    "for name, param in dti_model.named_parameters():\n",
    "    print(f\"{name:<40} {str(param.size()):<30} {param.requires_grad}\")\n",
    "print(\"=\"*86)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
